%%%%%%%% ICML 2018 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage[verbose=true,letterpaper]{geometry}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2018} with \usepackage[nohyperref]{icml2018} above.
%\usepackage{hyperref}
\usepackage[draft]{hyperref}
\usepackage[numbers]{natbib}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2018}


\usepackage{amsmath,amsbsy,amssymb,amsfonts,amsthm}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{color}
%\usepackage[algcompatible]{algpseudocode}
\usepackage{algorithm, algpseudocode}
%\usepackage{algorithmic}}
%\usepackage{algpseudocode}
\usepackage{wrapfig}
\def\compactify{\itemsep=0pt \topsep=0pt \partopsep=0pt \parsep=0pt}

\usepackage{setspace}
\usepackage{enumitem}
\usepackage{capt-of}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\mat}[1]{\bm{\mathit{#1}}}
\algdef{SE}[VARIABLES]{States}{EndStates}
   {\algorithmicvariables}
   {\algorithmicend\ \algorithmicvariables}
\algnewcommand{\algorithmicvariables}{\textbf{States}}
\algrenewcommand\Return{\State \algorithmicreturn{} }
\newcommand*{\AddNote}[4]{
    \begin{tikzpicture}[overlay, remember picture]
        \draw [decoration={brace,amplitude=0.5em},decorate,ultra thick,red]
            ($(#3)!(#1.north)!($(#3)-(0,1)$)$) --  
            ($(#3)!(#2.south)!($(#3)-(0,1)$)$)
                node [align=center, text width=2.5cm, pos=0.5, anchor=west] {#4};
    \end{tikzpicture}
}


\title{\tuner and the Art of Momentum Tuning}


\author{
  Jian Zhang, Ioannis Mitliagkas, Christopher R\'e \\
  Department of Computer Science\\
  Stanford University\\
  \texttt{\{zjian,imit,chrismre\}@cs.stanford.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\let\textproc\undefined
\newcommand\textproc{\textsc}
\newcommand{\tuner}{\textsc{YellowFin}\xspace}
\newcommand{\asynctuner}{closed-loop \textsc{YellowFin}\xspace}
\newcommand{\Asynctuner}{Closed-loop \textsc{YellowFin}\xspace}

\newcommand{\yell}[1]{#1}
\newcommand{\outline}[1]{}
\newcommand{\forjian}[1]{{\color{magenta}FOR JIAN: #1}}
\newcommand{\notes}[1]{{\color{green}NOTES: #1}}
\newcommand{\jianedits}[1]{#1}

\setlength{\parskip}{1.2ex}
\setlength{\parindent}{0pt}


% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2018}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{\tuner and the Art of Momentum Tuning}

\begin{document}
\maketitle
%\twocolumn[
%\icmltitle{\tuner and the Art of Momentum Tuning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2018
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}
%
%\begin{icmlauthorlist}
%\icmlauthor{Aeiau Zzzz}{equal,to}
%\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
%\icmlauthor{Cieua Vvvvv}{goo}
%\icmlauthor{Iaesut Saoeu}{ed}
%\icmlauthor{Fiuea Rrrr}{to}
%\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
%\icmlauthor{Aaoeu Iasoh}{goo}
%\icmlauthor{Buiui Eueu}{ed}
%\icmlauthor{Aeuia Zzzz}{ed}
%\icmlauthor{Bieea C.~Yyyy}{to,goo}
%\icmlauthor{Teoau Xxxx}{ed}
%\icmlauthor{Eee Pppp}{ed}
%\end{icmlauthorlist}
%
%\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
%\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
%\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}
%
%\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
%\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}
%
%% You may provide any keywords that you
%% find helpful for describing your paper; these are used to populate
%% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Machine Learning, ICML}
%
%\vskip 0.3in
%]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\noindent Hyperparameter tuning is one of the most time-consuming workloads in deep learning. 
State-of-the-art optimizers, such as AdaGrad, RMSProp and  Adam,
reduce this labor by adaptively tuning an individual learning rate for each variable.
Recently researchers have shown renewed interest in simpler methods like momentum SGD as they may yield better test metrics.
Motivated by this trend, we ask: can simple adaptive methods based on SGD perform as well or better? We revisit the momentum SGD algorithm and show that hand-tuning a single learning rate and momentum makes it competitive with Adam.
We then analyze its robustness to learning rate misspecification and objective curvature variation.
Based on these insights, we design \tuner, an automatic tuner for momentum and learning rate in SGD.
%In asynchronous-parallel training, \tuner optionally uses a momentum-sensing component with a negative-feedback loop to compensate for the dynamics of asynchrony on the fly.
\tuner optionally uses a negative-feedback loop to compensate for the momentum dynamics in asynchronous settings on the fly.
We empirically show that \tuner can converge in fewer iterations than Adam on ResNets and LSTMs for image recognition, language modeling and constituency parsing,
%	with a speedup of $0.8$x to $6.83$x in synchronous and up to $5.09$x in asynchronous settings.% on ResNet and LSTM models.
with a speedup of up to $3.28$x in synchronous and up to $2.69$x in asynchronous settings.% on ResNet and LSTM models.
\end{abstract}

\section{Introduction}
Accelerated forms of stochastic gradient descent (SGD), pioneered by
\citet{polyak1964some} and \citet{nesterov1983method}, are the de-facto
training algorithms for deep learning.
Their use requires a sane choice for their {\em hyperparameters}: 
typically a {\em learning rate} and {\em momentum parameter} \citep{sutskever2013importance}.
However, tuning hyperparameters is arguably the most time-consuming part of deep learning, with many papers outlining best tuning practices written
\citep{bengio2012practical,orr2003neural,bengio2012deep,bottou2012stochastic}.
Deep learning researchers have proposed a number of methods to deal with hyperparameter optimization, ranging from grid-search and 
smart black-box methods \citep{bergstra2012random,snoek2012practical}
to adaptive optimizers.
Adaptive optimizers aim to eliminate hyperparameter search by tuning on the fly for a single training run:
algorithms like AdaGrad \citep{duchi2011adaptive}, RMSProp \citep{tieleman2012lecture} and Adam \citep{kingma2014adam} use the magnitude of gradient elements to tune learning rates {\em individually for each variable} and  have been largely successful in relieving practitioners of tuning the learning rate. 
%\yell{
%This increased flexibility sounds great,
%however our experiments and recent analysis in literature \citep{wilson2017marginal} suggest that methods that adapt multiple learning rates, yield marginal benefits compared to momentum SGD.
%\citet{wilson2017marginal} argue that those methods also have worse generalization.
%We make another argument: adaptive methods also suffer from not tuning their momentum parameter.
%}

\begin{wrapfigure}[12]{R}{0.55\textwidth}
\vspace{-2.25em}
\begin{minipage}{1.0\linewidth}
\begin{figure}[H]
%	\includegraphics[width=1.\linewidth]{experiment_results/spotlight_default_adam.pdf}
	\includegraphics[width=0.99\linewidth]{experiment_results/spotlight.pdf}
%	\vspace{-1em}
	\caption{\tuner in comparison to Adam on a ResNet (CIFAR100, cf.\ Section~\ref{sec:experiments}) in synchronous and asynchronous settings.}
	\label{fig:spotlight}
%	\vspace{-0.5em}
\end{figure}
\end{minipage}
\end{wrapfigure}
Recently some researchers
 have started favoring simple momentum SGD over the previously mentioned adaptive methods~\citep{chen2016thorough,gehring2017convolutional}, often reporting better test scores \citep{wilson2017marginal}.
Motivated by this trend, we ask the question:
\emph{can simpler adaptive methods based on momentum SGD perform as well or better?}
%We revisit SGD with Polyak's momentum and a single learning rate for all variables.
We empirically show, with a hand-tuned learning rate, Polyak's momentum SGD achieves faster convergence than Adam for a large class of models.
We then formulate the optimization update as a dynamical system and study certain robustness properties of the momentum operator.
Inspired by our analysis, we design \tuner, an automatic hyperparameter tuner for momentum SGD.
\tuner simultaneously tunes the learning rate and momentum on the fly, and can handle the complex dynamics of asynchronous execution.
Our contribution and outline are as follows:
\begin{itemize}[leftmargin=2em]
\setlength\itemsep{0.2em}
\item
In Section~\ref{sec:momentum_operator}, we demonstrate examples where momentum offers convergence robust to learning rate misspecification and curvature variation in a class of non-convex objectives.
This robustness is desirable for deep learning.
It stems from a known but obscure fact:
the momentum operator's spectral radius is constant in a large subset of the hyperparameter space.
%\vspace{-1em}
\item
In Section~\ref{sec:sync_tuner}, we use these robustness insights and a simple quadratic model analysis to motivate the design of \tuner, an automatic tuner for momentum SGD.
\tuner uses on-the-fly measurements from the gradients to tune both a single learning rate and a single momentum.
%\setlength\itemsep{0.2em}
\item In Section~\ref{sec:stability}, we discuss common stability concerns related to the phenomenon of exploding gradients \citep{pascanu2013difficulty}.
We present a natural extension to our basic tuner, using adaptive gradient clipping, to stabilize training for objectives with exploding gradients.
%We present a natural extension to our basic tuner that stabilizes training for those objectives without sacrificing performance via adaptive gradient clipping.
\item In Section~\ref{sec:async_tuner} we present \asynctuner, suited for asynchronous training.
It uses a novel component for  measuring the total momentum in a running system, including any asynchrony-induced momentum, a phenomenon described in \cite{mitliagkas2016asynchrony}.
This measurement is used in a negative feedback loop to control the value of algorithmic momentum.% on the fly.

\end{itemize}


We provide a thorough empirical evaluation of the performance and stability of our tuner.
In Section~\ref{sec:experiments}, we demonstrate empirically that \yell{on ResNets and LSTMs}
\tuner can converge in fewer iterations compared to:
(i) hand-tuned momentum SGD (up to $1.75$x speedup);
and (ii) hand-tuned Adam ($0.77$x to $3.28$x speedup).
%and (ii) default Adam (0.8x to 6.83x speedup).
Under asynchrony, the closed-loop control architecture speeds up \tuner, 
making it up to $2.69$x faster than Adam. 
%making it up to $5.08$x faster than Adam. 
Our experiments include runs on $7$ different models, randomized over at least $3$ different random seeds. 
\tuner is stable and achieves consistent performance: the normalized sample standard deviation of test metrics varies from $0.05\%$ to $0.6\%$.
We released PyTorch and TensorFlow implementations
\footnote{TensorFlow: goo.gl/zC2rjG.  PyTorch: goo.gl/N4sFfs}%\footnote{PyTorch: https://github.com/AnonRepository/YellowFin\_Pytorch} 
that can be used as drop-in replacements for any optimizer.
%\footnote{ TensorFlow implementation: \url{https://github.com/JianGoForIt/YellowFin}}\footnote{PyTorch implementation: \url{https://github.com/JianGoForIt/YellowFin_Pytorch}.}.
%\tuner has also been implemented by various members of the ML community in Caffe2, Tensor2Tensor. 
\tuner has also been implemented in various other packages.
Its large-scale deployment in industry has taught us important lessons about stability; we discuss those challenges and our solution in Section~\ref{sec:stability}.
We conclude with related work and discussion in Section~\ref{sec:related} and~\ref{sec:discussion}.


\vspace{-0.1em}
\section{The momentum operator}
\label{sec:momentum_operator}

\newcommand{\gc}{generalized curvature\xspace}
\newcommand{\Gc}{Generalized curvature\xspace}
\vspace{-0.15em}
In this section, we identify the main technical insight behind the design of \tuner:
%After preliminaries on momentum gradient descent, 
 gradient descent with momentum can exhibit linear convergence robust to learning rate misspecification and to curvature variation.
The robustness to learning rate misspecification means tolerance to a less-carefully-tuned learning rate.
On the other hand, the robustness to curvature variation means empirical linear convergence on a class of non-convex objectives with varying curvatures.
After preliminary on momentum, 
we discuss these two properties desirable for deep learning objectives.
%In this section, we analyze the main insights for designing \tuner, showing that momentum is robust to learning rate misspecification and curvature variation for a class of non-convex objectives.
%%momentum is robust to learning rate misspecification and curvature variation for a class of non-convex objectives.

%In this section, we analyze two properties of momentum: it is robust to learning rate misspecification and to curvature variation for a class of non-convex objectives, both desirable for deep learning.


%\vspace{-0.5em}
\subsection{Preliminaries}
\label{sec:robust_preliminaries}
We aim to minimize some objective $f(x)$.
In machine learning, $x$ is referred to as {\em the model} and the objective is some {\em loss function}.
A low loss implies a well-fit model.
Gradient descent-based procedures use the gradient of the objective function, $\nabla f(x)$, to update the model iteratively.
These procedures can be characterized by the convergence rate with respect to the distance to a minimum.
\begin{definition} [Convergence rate]
	Let $x^*$ be a local minimum of $f(x)$ and $x_t$ denote the model after $t$ steps of an iterative procedure. %$f(x):\mathbb{R}^d \mapsto \mathbb{R}$ 
%	and $x_t$ be the model after $t$ steps in an iterative minimization procedure. 
The iterates converge to $x^*$ with linear rate $\beta$,
	if \[ \| x_{t} - x^* \| = O(\beta^t \| x_0 - x^* \|).\]
\end{definition}
Polyak's momentum gradient descent \citep{polyak1964some} is one of these iterative procedures, given by
%Polyak's momentum gradient descent update \citep{polyak1964some} is given by
\begin{align}
	x_{t+1}  &= x_t - \alpha \nabla f(x_t) + \mu (x_t - x_{t-1}),
	\label{eqn:momentum_gd}
\end{align} 
where $\alpha$ denotes a single learning rate and $\mu$ a single momentum for all model variables.   
Momentum's main appeal is its established ability to {accelerate convergence} \citep{polyak1964some}. 
On a $\gamma$-strongly convex $\delta$-smooth function with condition number $\kappa=\delta/\gamma$, the optimal convergence rate of gradient descent without momentum
is $O(\frac{\kappa-1}{\kappa+1})$~\citep{nesterov2013introductory}.
On the other hand, for certain classes of strongly convex and smooth functions, like quadratics,
 the optimal momentum value,
\vspace{-0.5em}
\begin{equation}
	\mu^* = \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\right)^2,
	\label{eqn:optimal_momentum}
\end{equation}
yields the optimal accelerated linear convergence rate $O(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})$.
{\em This guarantee does not generalize to arbitrary strongly convex smooth functions} \citep{lessard2016analysis}.
Nonetheless, this linear rate can often be observed in practice even on non-quadratics (cf. Section~\ref{sec:robust_properties}).

{\bf Key insight:}
%{\bf The main insight we expose has been hidden in proofs of the optimal convergence rate:}
%{\bf Main insights:}
%On quadratics with different curvature along different axes, when using \emph{gradient descent without momentum}, 
%we get different convergence rate to the minimum along the axes; the achieved overall rate, $O(\frac{\kappa-1}{\kappa+1})$, is just the slowest rate over all axes. 
%In sharp contrast, Polyak's momentum in \eqref{eqn:momentum_gd} converges to a minimum with the accelerated linear rate $\sqrt{\mu^*}$ from {\em all directions},
%when we use the optimal momentum value, $\mu^*$.
%This can be seen by analyzing a quadratic, though it might not generalize to all strongly convex, smooth functions.
%Concretely, consider a strongly convex quadratic, and let $x_{i, t}$ and $x_i^*$ be the i-th coordinates of $x_t$ and $x^*$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% backup discussion %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%yields the optimal accelerated linear convergence rate $O(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1})$ 
%%(i.e. $O(\sqrt{\mu^*})$)
%%}.
%%\footnote{
%%{
%%\yell{
%This guarantee does not generalize to arbitrary non-quadratics \citep{lessard2016analysis}.
%Nonetheless, the linear convergence can be observed in practice (cf. Section~\ref{sec:robust_properties}).
%%}
%%}
%
%{\bf A fact is often hidden away in proofs of the guarantee:} 
%{\bf For any momentum $\mu > \mu^*$, there exists a range of different learning rate $\alpha$
%\emph{for any momentum $\mu \geq \mu^*$, 
%using a range of different learning rate,
Consider a quadratic objective with condition number $\kappa > 1$.
Even though its curvature is different along the different directions,
% and different curvatures along different axes.
%On this quadratic, 
%when using \emph{gradient descent without momentum}, 
%we get different convergence rate along the axes; the achieved overall rate, $O(\frac{\kappa-1}{\kappa+1})$, is just the slowest rate over all axes.
Polyak's momentum gradient descent, with $\mu \geq \mu^*$, achieves \emph{the same linear convergence rate $\sqrt{\mu}$ along all directions}. Specifically, let $x_{i, t}$ and $x_i^*$ be the i-th coordinates of $x_t$ and $x^*\!$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%the optimum and the model after $t$ iterations respectively, 
For any $\mu \geq \mu^*$ with an appropriate learning rate, the update in~\eqref{eqn:momentum_gd} can achieve $| x_{i, t} - x_i^* | \leq \sqrt{\mu}^t | x_{i,0} - x_i^* |$ simultaneously along all axes $i$.
This insight has been hidden away in proofs.
%The same constant convergence rate holds along any arbitrary projection.

%%This insight
%This hidden fact 
%reveals a common linear convergence rate on quadratics along all axes.
In this quadratic case, curvature is different across different axes, but remains {constant on any one-dimensional slice}. 
In the next section (Section~\ref{sec:robust_properties}), we extend this insight to non-quadratic one-dimensional functions.
We then present the \emph{main technical insight behind the design of \tuner: 
%\emph{
similar linear convergence rate $\sqrt{\mu}$ can be achieved in a class of one-dimensional non-convex objectives where curvature varies}; this linear convergence behavior is robust to learning rate misspecification and to the varying curvature. These \emph{robustness properties} are behind a tuning rule for learning rate and momentum in Section~\ref{sec:robust_properties}. We extend this rule to handle SGD noise and generalize it to multidimensional objectives in Section~\ref{sec:sync_tuner}.

%In the next subsection, we unpack, extend and explains this insight as two properties on the convergence behavior of Polyak's momentum gradient descent:
%(i) robustness to learning rate misspecification: 
%%this is the dual view to the curvature robustness described above and means 
%convergence can be tolerant to less-carefully-tuned learning rate;
%(ii) robustness to the varying curvature {\em along a one-dimensional objective (or a scalar-slice of multidimensional objective)}:
%we already discussed curvature variation along different directions on quadratics, as well as the role of the condition number;
%next we will see how curvature and condition number admit a meaningful generalization that allows us to characterize convergence on one-dimensional objectives with varying curvatures.
%%We will see that we can observe constant linear rates and acceleration even on one-dimensional objectives by using our rule to tune a positive value for the momentum parameter.
%%Then we extend this tuning rule to multi-dimensional objectives.

%(e.g.\ the `$\mu=0.0$` curve in Figure~\ref{fig:lr_robustness})
%on quadratics,
%$\mu^*$ is the smallest value for which the update in~\eqref{eqn:momentum_gd} can achieve \emph{the same linear convergence rate 1) along all axes with varying curvatures, 2) over a range of different learning rate}.  For quadratics, curvature varies across axes but remains constant on the slice along each axis. In Section~\ref{sec:robust_properties}, we show with examples that similar robust linear convergence can be achieved in a class of one dimensional non-convex objectives where curvature varies.
%%%%%%%%%%%%%%%%%%%%%%%%%%% backup discussion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Then {for any $\mu\!\geq\!\mu^*\!$}, there exists {a range of $\alpha$}, such that $| x_{i, t} - x_i^* | \leq\sqrt{\mu}^t | x_{i,0} - x_i^* |$ for all axis $i$. For quadratics, curvature varies across axes but remains constant on a slice along each axis. In Section~\ref{sec:robust_properties}, we show with examples that \emph{similar robust linear convergence can be achieved in a class of one dimensional non-convex objectives where curvature varies}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%{\bf This fact is often hidden away in proofs}. 
%We shed light on some of its previously unknown implications in Section~\ref{sec:robust_properties}.

% {\bf This property is often hidden away in proofs}. We extend the intuition of this property from quadratics to one dimensional non-quadratic in Section~\ref{sec:robust_properties}. 

%We shed light on some of its previously unknown implications in Section~\ref{sec:robust_properties}. % and use them in our tuner in Section~\ref{sec:sync_tuner}.

\subsection{Robustness properties of the momentum operator}
%\vspace{-0.2em}
\label{sec:robust_properties}
In this section, we analyze the dynamics of momentum on a class of one-dimensional, non-convex objectives.
We first introduce the notion of {\em generalized curvature} and use it to describe the momentum operator.
Then we discuss the robustness properties of the momentum operator.
%, and extract a tuning rule for learning rate $\alpha$ and momentum $\mu$. 
%\forjian{Should we explicitly mention what is the robustness?}
%In the last section, we discuss the same linear convergence rate of Polyak's momentum gradient descent on different axes with different curvatures for quadratics. 
%For quadratics, the curvature along each axes is constant. 
%In this section, we extend the discussion to a simple class of one dimensional non-convex functions with curvature variations.
%%In this section we analyze the dynamics of momentum on a simple class of one dimensional, non-convex objectives.
%We first introduce the notion of {\em generalized curvature} and use it to describe the momentum operator.
%Then we discuss the robustness properties of the momentum operator: achieving linear convergence rate 

Curvature along different directions is encoded in the different eigenvalues of the Hessian. 
%The classic condition number captures the local variation of curvature, i.e. the eigenvalues of local Hessian.
It is the only feature of a quadratic needed to characterize the convergence of gradient descent. Specifically, gradient descent achieves a linear convergence rate $|1 - \alpha h_c|$ on one-dimensional quadratics with constant curvature $h_c$. 
On one-dimensional \emph{non-quadratic objectives with varying curvature}, this neat characterization is lost.
We can recover it by defining a new kind of ``curvature'' with respect to a specific minimum.

\begin{definition}[\Gc]
\label{def:generalized_curvature}
Let $x^*$ be a local minimum of $f(x):\mathbb{R}\rightarrow\mathbb{R}$.
Generalized curvature with respect to $x^*$, denoted by $h(x)$, satisfies the following.
\begin{equation}
	 f'(x) = h(x) (x - x^*). 
	\label{eqn:generalized_curvature}
\end{equation}
%We call $h(x)$ the {\em generalized curvature}.
\end{definition}
\Gc describes, in some sense, \emph{non-local curvature} with respect to minimum $x^*$.
It coincides with curvature on quadratics.
On non-quadratic objectives, it characterizes the convergence behavior of gradient descent-based algorithms.
Specifically, we recover the fact that starting at point $x_t$, distance from minimum $x^*$ is reduced by $|1-\alpha h(x_t)|$ in one step of gradient descent.
%It can captures the longer-range, non-local variations of curvature.
%For quadratic objectives, it coincides with the standard definition of curvature, and is the sole quantity related to the objective that influences the dynamics of gradient descent.
%For example, the contraction of a gradient descent step is $1-\alpha h(x_t)$.
%Using a state-space augmentation, we can rewrite the momentum update of~\eqref{eqn:momentum_gd}, and define the {\em momentum operator} $\mat{A}_t$ at time $t$ as
%\begin{equation}
%\begin{aligned}
%{\begin{pmatrix}
%x_{t+1} - x^*\\
%x_t - x^* \\
%\end{pmatrix}}
%&=
%{\begin{bmatrix}
%1-\alpha h(x_t) + \mu & - \mu \\
%1 & 0 \\
%\end{bmatrix}}
%{\begin{pmatrix}
%x_t - x^* \\
%x_{t-1} - x^*\\
%\end{pmatrix}} \\
%&\triangleq
%\mat{A}_t
%{\begin{pmatrix}
%x_t - x^* \\
%x_{t-1} - x^*\\
%\end{pmatrix}}.
%\label{equ:one_dim_22_rec}
%\end{aligned}
%\end{equation}
Using a state-space augmentation, we can rewrite the momentum update of~\eqref{eqn:momentum_gd} as
\begin{equation}
\begin{aligned}
{\begin{pmatrix}
x_{t+1} - x^*\\
x_t - x^* \\
\end{pmatrix}}
&= \mat{A}_t
{\begin{pmatrix}
x_t - x^* \\
x_{t-1} - x^*\\
\end{pmatrix}} %\\
%&\triangleq
%\mat{A}_t
%{\begin{pmatrix}
%x_t - x^* \\
%x_{t-1} - x^*\\
%\end{pmatrix}}.
\label{equ:one_dim_22_rec}
\end{aligned}
\end{equation}
where the {\em momentum operator} $\mat{A}_t$ at time $t$ is defined as
\begin{equation}
	\mat{A}_t \triangleq {\begin{bmatrix}
	1-\alpha h(x_t) + \mu & - \mu \\
	1 & 0 \\
	\end{bmatrix}}
\end{equation}



\begin{lemma}[Robustness of the momentum operator]
\label{lem:robustness}
Assume that generalized curvature $h$ and hyperparameters $\alpha,\mu$ satisfy
\begin{align}
{(1-\sqrt{\mu})^2} &\leq \alpha h(x_t) \leq {(1+\sqrt{\mu})^2}.
\label{eqn:robust_region}
\end{align}
%then the spectral radius of the momentum operator, which describes convergence behavior, only depends on  momentum: $	\rho(\mat{A}_t) = \sqrt{\mu}
Then as proven in Appendix~\ref{sec:proof_robustness}, the spectral radius of the momentum operator at step $t$ depends solely on the  momentum parameter: $	\rho(\mat{A}_t) = \sqrt{\mu}$, for all $t$. 
The inequalities in \eqref{eqn:robust_region} define the {\bf robust region}, the set of learning rate $\alpha$ and momentum $\mu$ achieving this $\sqrt{\mu}$ spectral radius.
\end{lemma}
%The spectral radius of an operator, $A_t$ gives the convergence rate when the same operator is applied multiple times, i.e.\ $\mat{A}_t\cdots\mat{A}_t$.
%In this case, different operators are applied, $\mat{A}_t\cdots\mat{A}_1$,
%all of which have the same spectral radius. 
%This means that, as already discussed, {\em we do not give convergence rate guarantees}.
%However, we do show examples where the time-homogeneous spectral radius guaranteed by Lemma~\ref{lem:robustness} translates to constant linear rates on non-convex objectives and predicts the convergence rate of most model variables in deep learning objectives (Figure~\ref{fig:curvature_robustness}).
%Next we discuss two useful implications of this result.
%%Thus we explain Lemma~\ref{lem:robustness} as the {\bf robustness properties} of momentum operator: \emph{time-homogeneous spectral radii $\sqrt{\mu}$ implies asymptotic linear convergence robust with respect to learning rate and to curvature variations}.
%%Note the spectral radius of the composition of operators $\mat{A}_t\cdots\mat{A}_1$, all with spectral radius $\sqrt{\mu}$, does not always follow the asymptotics of $\sqrt{\mu}^t$.
%%In other words, {\em we do not provide a convergence rate guarantee}. Instead, we show the robustness properties with examples.

%%%%%%%%%%% backup discussion %%%%%%%%%%%%%%%%%
%The proof is given in Appendix~\ref{sec:proof_robustness}.
We know that the spectral radius of an operator, $\mat{A}$, describes its asymptotic behavior when applied multiple times: $\| A^t x \| \approx O(\rho(\mat{A})^t)$.\footnote{
For any $\epsilon > 0$, there exists a matrix norm $\|\cdot\|$ such that $\|\mat{A}\| \leq \rho(A) + \epsilon$~\citep{simon2012spectralradius}.
}
%\footnote{For any $\epsilon > 0$, there exists a matrix norm $\|\cdot\|$ such that $\|\mat{A}_t\cdots\mat{A}_1\| \leq \rho(\mat{A}_t\cdots\mat{A}_1) + \epsilon$~\citep{simon2012spectralradius}.}.
Unfortunately, the same does not always hold for the composition of {\em different } operators, even if they have the same spectral radius, $\rho(\mat{A}_t)=\sqrt{\mu}$.
It is not always true that $\| \mat{A}_t\cdots\mat{A}_1 x\| = O(\sqrt{\mu}^t)$.
%According to~\eqref{equ:one_dim_22_rec}, the operator composition $\mat{A}_t\cdots\mat{A}_1$ describes the evolution of distance $|x_t -x^*|$.
%Consequently, the spectral radius of this composition $\mat{A}_t\cdots\mat{A}_1$ is closely related to the convergence behavior, namely the asymptotic relation between $|x_t -x^*|$ and the initial distance $|x_0 - x^*|$. 
%%The spectral radius of $\mat{A}_t$ can describe the evolution of distance $|x_t -x^*|$ in~\eqref{equ:one_dim_22_rec}. Consequently, the spectral radius of the composition $\mat{A}_t\cdots\mat{A}_1$ is closely related\footnote{For any $\epsilon > 0$, there exists a matrix norm $\|\cdot\|$ such that $\|\mat{A}_t\cdots\mat{A}_1\| \leq \rho(\mat{A}_t\cdots\mat{A}_1) + \epsilon$~\citep{simon2012spectralradius}.} to the convergence behavior, namely the asymptotic relationship between $|x_t -x^*|$ and the initial distance $|x_0 - x^*|$. 
%Unfortunately given time-homogenous spectral radii $\sqrt{\mu}$, the spectral radius of the composition $\mat{A}_t\cdots\mat{A}_1$ does not always follow the asymptotics of $\sqrt{\mu}^t$.
However, a homogeneous spectral radius often yields the $\sqrt{\mu}^t$ rate empirically.
In other words, {\em this linear convergence rate is not guaranteed}.
Instead, we demonstrate examples to expose the {\bf robustness properties}: \emph{if the learning rate $\alpha$ and momentum $\mu$ are in the robust region, 
the homogeneity of spectral radii can empirically yield linear convergence with rate $\sqrt{\mu}$; this behavior is robust with respect to learning rate misspecification and to varying curvature}.
%Thus Lemma~\ref{lem:robustness} can imply the {\bf robustness properties} of momentum operator: \emph{time-homogeneous spectral radius $\sqrt{\mu}$ of $\mat{A}_t$ imply empirical linear convergence; this behavior is robust with respect to learning rate and to curvature variations}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{wrapfigure}[12]{r}{0.25\textwidth}
%\vspace{-2.5em}
%\begin{minipage}{1.0\linewidth}
%\begin{figure}[H]
%  \includegraphics[width=\linewidth]{figures/spectral_radii}
%%  \vspace{-0.75em}
%\caption{
%Momentum operator on scalar quadratic.
%}
%\label{fig:lr_robustness}
%\end{figure}
%\end{minipage}
%\end{wrapfigure}
%\vspace{-0.5em}
\paragraph{Momentum is robust to learning rate misspecification}
\label{sec:lr_robustness}
For a one-dimensional quadratic with curvature $h$,
we have generalized curvature $h(x)=h$ for all $x$. Lemma~\ref{lem:robustness} implies the spectral radius $\rho(\mat{A}_t)\!=\!\sqrt{\mu}$ if
\begin{align}
{(1-\sqrt{\mu})^2/h} &\leq \alpha \leq {(1+\sqrt{\mu})^2/h}.
\label{eqn:lr_robustness}
\end{align}

\begin{wrapfigure}[14]{R}{0.23\textwidth}
\vspace{-2.5em}
%\hspace{-0.5em}
\begin{minipage}{1.0\linewidth}
\begin{figure}[H]
  \includegraphics[width=\linewidth]{figures/spectral_radii}
  \vspace{-1.5em}
\caption{
Spectral radius of momentum operator on scalar quadratic
for varying $\alpha$.
}
\label{fig:lr_robustness}
\end{figure}
\end{minipage}
\end{wrapfigure}
In Figure~\ref{fig:lr_robustness}, we plot $\rho(\mat{A}_t)$ for different $\alpha$ and $\mu$ when $h\!=\!1$.
The solid line segments correspond to the robust region.
As we increase momentum, a linear rate of convergence, $\sqrt{\mu}$, is robustly achieved by an ever-widening range of learning rates:
higher values of momentum are more robust to learning rate mispecification.
%We also note that for objectives with large condition number, higher values of momentum are {\em both faster and more robust}.

{\bf This property influences the design of our tuner:}
\emph{more generally for a class of one-dimensional non-convex objectives},
as long as the learning rate $\alpha$ and momentum $\mu$ are in the {\em robust region}, i.e.\ satisfy \eqref{eqn:robust_region} at every step, then
{\em momentum operators at all steps $t$ have the same spectral radius}.
In the case of quadratics, this implies a convergence rate of $\sqrt{\mu}$, independent of the learning rate.
%%%%%%%%%%%%%%%%%% backup discussion %%%%%%%%%%%%%%%%%%%%%
%We also note that for objectives with large condition number, higher values of momentum can be {\em more robust to learning rate variations}.
%{\bf This property influences the design of our tuner:} as long as \eqref{eqn:lr_robustness} is satisfied, we are in the {\em robust region} and 
%expect the same convergence rate of $\sqrt{\mu}$ for quadratics, independent of the learning rate.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Having established that, we can just focus on optimally tuning momentum.


%\vspace{-0.5em}
\paragraph{Momentum is robust to varying curvature}
\label{sec:curvature_robustness}

As discussed in Section~\ref{sec:robust_preliminaries}, the intuition hidden in classic results
is that for certain strongly convex smooth objectives, 
%momentum larger than 
momentum at least as high as
the value in \eqref{eqn:optimal_momentum} can achieve the same rate of linear convergence along all axes with different curvatures. 
We extend this intuition to certain one-dimensional non-convex functions with varying curvatures along their domains; we discuss the generalization to multidimensional cases in Section~\ref{sec:tuner}.
%where steepness---and, as a result, contractivity---vary as we move along.
Lemma~\ref{lem:robustness} guarantees constant, time-homogeneous spectral radii for momentum operators $\mat{A}_t$ 
assuming \eqref{eqn:robust_region} is satisfied at every step. 
This assumption motivates a ``long-range'' extension of the condition number.
%%%%%%%%%%%%%%%%%%% backup discussion %%%%%%%%%%%%%%%%%%
%is that for a subset of strongly convex smooth objectives, momentum greater than the value in \eqref{eqn:optimal_momentum} guarantees the same rate of linear convergence along all axes with different curvatures. 
%We extend this intuition to certain one dimensional, non-convex functions where curvature varies along this slice.
%Lemma~\ref{lem:robustness} guarantees a constant, time-homogeneous spectral radius for the momentum operators $\mat{A}_t$ if 
%\eqref{eqn:robust_region} is satisfied at every step. 
%This motivates an extension of the condition number.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{definition}[Generalized condition number]
We define the generalized condition number (GCN) with respect to a local minimum $x^*$ of a scalar function, $f(x):\mathbb{R}\rightarrow \mathbb{R}$, to be the dynamic range of its generalized curvature $h(x)$:
\begin{equation}
	\nu = \frac{\sup_{x \in dom(f)} h(x)}{ \inf_{x \in dom(f)} h(x)}
\end{equation}
\end{definition}
The GCN captures variations in generalized curvature along a scalar slice.
From Lemma~\ref{lem:robustness} we get
\begin{equation}
%\begin{aligned}
\begin{gathered}
	\mu \geq \mu^* = \left(\frac{\sqrt{\nu}-1}{\sqrt{\nu}+1}\right)^2, \\
%	\quad
	\frac{(1-\sqrt{\mu})^2}{\inf_{x \in dom(f)}h(x)} \leq \alpha \leq \frac{(1+\sqrt{\mu})^2}{\sup_{x \in dom(f)}h(x)}
	\label{eqn:noiseless_tuning_rule}
\end{gathered}
%\end{aligned}
\end{equation}
as the description of the robust region. The momentum and learning rate satisfying~\eqref{eqn:noiseless_tuning_rule} guarantees a homogeneous spectral radius of $\sqrt{\mu}$ for all $\mat{A}_t$.
Specifically, $\mu^*$ is the smallest momentum value that allows for homogeneous spectral radii.
%The spectral radius of an operator describes its asymptotic convergence behavior. 
%However, the product of a sequence of operators $\mat{A}_t\cdots\mat{A}_1$ all with spectral radius $\sqrt{\mu}$ does not always follow the asymptotics of $\sqrt{\mu}^t$.
%In other words, {\em we do not provide a convergence rate guarantee}.
%Instead, we provide evidence in support of this intuition. 
We demonstrate with examples that \emph{homogeneous spectral radii suggest an empirical linear convergence behavior on a class of non-convex objectives}. In Figure~\ref{fig:curvature_robustness}(a), the non-convex objective,
composed of two quadratics with curvatures $1$ and $1000$, has a GCN of $1000$.
Using the tuning rule of \eqref{eqn:noiseless_tuning_rule}, and running the momentum algorithm (Figure~\ref{fig:curvature_robustness}(b)) practically yields the linear convergence predicted by Lemma~\ref{lem:robustness}.
%In Figures~\ref{fig:curvature_robustness}(c,d) we demonstrate that for an LSTM,
%the majority of model variables follow a $\sqrt{\mu}$ convergence rate.
In Figures~\ref{fig:curvature_robustness}(c,d), we demonstrate an LSTM as another example. As we increase the momentum value (the same value for all variables in the model), more model variables follow a $\sqrt{\mu}$ convergence rate.
In these examples, \emph{the linear convergence is robust to the varying curvature of the objectives}. \textbf{This property influences our tuner design:}
{in the next section, we extend the tuning rules of \eqref{eqn:noiseless_tuning_rule} to handle SGD noise; 
we generalize the extended rule to multidimensional cases as the tuning rule in \tuner}.
%the multidimensional generalization of the extended rule is the tuning rule in \tuner}.
%in the next section we use the tuning rules of \eqref{eqn:noiseless_tuning_rule} in \tuner,
%generalized appropriately to handle SGD noise.






\begin{figure*}[t]
\centering
\vspace{-0.5em}
\begin{tabular}{c c c c}
  \includegraphics[width=0.225\linewidth]{figures/non_convex_toy} &
  \includegraphics[width=0.235\linewidth]{figures/non_convex_constant_rate} &
  \includegraphics[width=0.235\linewidth]{figures/constant_rate_09} &
  \includegraphics[width=0.185\linewidth]{figures/constant_rate_099} \\
  (a) & (b) & (c) &(d)
\end{tabular}
\vspace{-0.5em}
\caption{(a) Non-convex toy example;
(b) linear convergence rate achieved empirically on the example in (a) tuned according to \eqref{eqn:noiseless_tuning_rule};
(c,d)
LSTM on MNIST: as momentum increases from $0.9$ to $0.99$, the global learning rate and momentum falls in robust regions of more model variables. The convergence behavior (shown in grey) of these variables follow the robust rate $\sqrt{\mu}$ (shown in red).}
\vspace{-0.25em}
\label{fig:curvature_robustness}
\end{figure*}











\input{tuner}

\input{stability}
%\section{Stability on non-smooth objectives}
%\label{sec:stability}
%
%\begin{figure}[t]
%\centering
%  \includegraphics[width=0.8\linewidth]{experiment_results/clipping_example.pdf} 
%\caption{An LSTM with hidden units exhibits exploding gradients.
%The proposed adaptive threshold for gradient clipping (blue line) keeps instabilities contained.
%The network is a small variation of the architecture presented in \citep{zhu2016trained}.}
%\label{fig:stability}
%\end{figure}
%
%
%Section~\ref{sec:sync_tuner} describes the core of the \tuner tuner.
%It uses the basic tuning rules extracted from a noisy quadratic model.
%To engineer our implementation on arbitrary objectives, we calculate smoothed, rough approximations of curvature ranges, a distance form a local minimum and a gradient variance.
%However, certain neural network objectives can involve arbitrary non-linearities, and large Lipschitz constants \citep{szegedy2013intriguing}.
%Furthermore, the process of training them is inherently non-stationary, with the landscape abruptly switching from flat to steep areas. 
%In particular, the objective functions associated with RNNs   with hidden units can exhibit occasional but very steep slopes \citep{pascanu2013difficulty}.
%So, it is not always safe to assume that the statistics we calculated so far will accurately represent the objective function in the next step.
%
%In Figure~\ref{fig:stability}, we present such an example of an LSTM that exhibits this 'exploding gradient' issue.
%To deal with this issue, we propose a very natural addition to our basic tuner, that performs {\em adaptive gradient clipping}. 
%Gradient clipping has been established in literature as a standard---almost necessary---tool for training such objectives \citep{pascanu2013difficulty,Goodfellow-et-al-2016,gehring2017convolutional}. 
%However, the classic tradeoff between adaptivity and stability applies: 
%setting a clipping threshold that is too low can hurt performance;
%setting it to be high, can compromise stability.
%\tuner, keeps running estimates of extremal gradient magnitude squares, $h_{max}$ and $h_{min}$ in order to estimate a generalized condition number.
%We posit that $\sqrt{h_{max}}$ is an ideal gradient norm threshold for adaptive clipping.
%In order to improve robustness to extreme gradient spikes, like the ones in Figure~\ref{fig:stability}, we also limit the growth rate of the envelope $h_{max}$ as follows:
%\begin{equation}
% h_{max} 
% \leftarrow
% \beta \cdot h_{max}
% 	+ (1-\beta) \cdot \textrm{min}\left\{
% 		h_{max,t}, 100 \cdot h_{max}
% 	\right\}
%\end{equation}
%\begin{table}
%\centering
%\begin{tabular} { c | c | c | c}
%\toprule
%	& Default w/o clip. & Default w/ clip. & YF \\
%\midrule
%\midrule
%	Validation loss & diverge & 2.86 & 2.75 \\
%	Validation BLEU4 & diverge & 30.75 & 31.59 \\ 
%\bottomrule
%\end{tabular}
%\caption{German-English translation performance using convolutional sequence to sequence learning.}
%\label{tab:conv_seq}
%\end{table}
%
%We demonstrate the performance of YellowFin with adaptive clipping on IWSLT 2014 German-English translation task using convolutional sequence to sequence learning~\citep{gehring2017convolutional}. We train the model for 120 epochs and report the best validation loss, as well as the best BLEU4 score. We follow the default optimizer setting in~\citep{gehring2017convolutional}, where manually set strict clipping is applied before performing SGD with learning rate 0.25 and momentum 0.99. The optimizer diverges when the clipping is removed. In Table~\ref{tab:conv_seq}, we can see YellowFin, with adaptive clipping, can outperform the default optimizer with manually set clipping in both validation loss and BLEU4 score.
%Our proposed adaptive clipping helps stabilize difficult objectives, without sacrificing performance.
%In Appendix~\ref{sec:infl_clip} we demonstrate that on models that do not exhibit instabilities, our clipping does not hurt performance.
%
%%Thinking fast and slow approach:
%%- slow layer, the basic tuner we described
%%- fast layer: applies clipping based on the statistics estimated
%%- *and* we don’t let the estimates grow too quickly
%



\input{async_yf}


\input{experiments.tex}

\vspace{-0.5em}
\section{Related work}
\label{sec:related}
\vspace{-0.45em}
%Many techniques have been proposed on tuning hyperparameters for optimizers.~\citet{bergstra2012random} investigate random search for general tuning of  hyperparameters. 
%Bayesian approaches~\citep{snoek2012practical} model evaluation metrics as samples from a Gaussian process guiding optimal hyperparameter search. 
%Another trend is the adaptive methods which require less manual tuning than SGD:
%Adagrad~\citep{duchi2011adaptive} is one of the first method with per-dimension learning rate, followed by RMSProp~\citep{tieleman2012lecture} and Adam~\citep{chilimbi2014project} using different learning rate rules. 
%\citet{schaul2013no} use a noisy quadratic model similar to ours to extract learning rate tuning rule in Vanilla SGD.
%However their approach does not use momentum which is essential in training modern neural networks. Existing adaptive momentum approach either only consider the non-stochastic setting~\citep{graepel2002stable,rehman2011effect,hameed2016back,swanston1994simple,ampazis2000levenberg,qiu1992accelerated} or only analyze stochasticity with $O(1/t)$ learning rate. In the contrast, we aim at practical momentum adaptivity for stochastically  training of modern neural networks, as well as simultaneously presenting learning rate adaptivity.  
Many techniques have been proposed on tuning hyperparameters for optimizers. General hyperparameter tuning approaches, such as random search~\citep{bergstra2012random} and Bayesian approaches~\citep{snoek2012practical, hutter2011sequential}, can directly tune optimizers.  
As another trend, adaptive methods, including AdaGrad~\citep{duchi2011adaptive}, RMSProp~\citep{tieleman2012lecture} and Adam~\citep{kingma2014adam}, uses per-dimension learning rate. 
%They typically require less manual tuning than SGD. 
\citet{schaul2013no} use a noisy quadratic model similar to ours to tune the learning rate in Vanilla SGD.
However they do not use momentum which is essential in training modern neural nets. Existing adaptive momentum approach either consider the deterministic setting~\citep{graepel2002stable,rehman2011effect,hameed2016back,swanston1994simple,ampazis2000levenberg,qiu1992accelerated} or only analyze stochasticity with $O(1/t)$ learning rate~\citep{leen1994optimal}. In contrast, we aim at practical momentum adaptivity for stochastically training neural nets.  

\vspace{-0.5em}
\section{Discussion}
\label{sec:discussion}
\vspace{-0.45em}
We presented \tuner, the first optimization method that automatically tunes momentum as well as the learning rate of momentum SGD. 
\tuner outperforms the state-of-the-art adaptive optimizers on a large class of models both in synchronous and asynchronous settings.
It estimates statistics purely from the gradients of a running system,
and then tunes the hyperparameters of momentum SGD based on noisy, local quadratic approximations.
As future work, we believe that more accurate curvature estimation methods,
like the $bbprop$ method~\citep{martens2012estimating} can further improve \tuner.
We also believe that our closed-loop momentum control mechanism in Section~\ref{sec:async_tuner} 
could accelerate other adaptive methods in asynchronous-parallel settings.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{arxiv,iclr2018_conference}
\bibliographystyle{icml2018}


\appendix 
\input{main_lemma}
%\input{multi_dim}
%\input{dist_small_lstm}
\input{opt}
%\input{async_yf}
\input{practical_impl}
\input{clip_influence}
\input{async_app}
\input{model_spec}
\input{exp_spec}
%\input{test_perp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\appendix
%\section{Do \emph{not} have an appendix here}
%
%\textbf{\emph{Do not put content after the references.}}
%%
%Put anything that you might normally include after the references in a separate
%supplementary file.
%
%We recommend that you build supplementary material in a separate document.
%If you must create one PDF and cut it up, please be careful to use a tool that
%doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
%pdftk usually works fine. 
%
%\textbf{Please do not use Apple's preview to cut off supplementary material.} In
%previous years it has altered margins, and created headaches at the camera-ready
%stage. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
