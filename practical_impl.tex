\section{Practical implementation}
\label{sec:practical_impl}
In Section~\ref{sec:oracles}, we discuss estimators for learning rate and momentum tuning in \tuner. In our experiment practice, we have identified a few practical implementation details which are important for improving estimators. Zero-debias is proposed by~\citet{kingma2014adam}, which accelerates the process where exponential average adapts to the level of original quantity in the beginning. We applied zero-debias to all the exponential average quantities involved in our estimators. In some LSTM models, we observe that our estimated curvature may decrease quickly along the optimization process. In order to better estimate extremal curvature $h_{\max}$ and $h_{\min}$ with fast decreasing trend, we apply zero-debias exponential average on the logarithmic of $h_{\max, t}$ and $h_{\min, t}$, instead of directly on $h_{\max, t}$ and $h_{\min, t}$. Except from the above two techniques, we also implemented the slow start heuristic proposed by~\citep{schaul2013no}. More specifically, we use $\alpha = \min\{\alpha_t, t \cdot \alpha_t / (10 \cdot w) \}$ as our learning rate with $w$ as the size of our sliding window in $h_{\max}$ and $h_{\min}$ estimation. It discount the learning rate in the first $10 \cdot w$ steps and helps to keep the learning rate small in the beginning when the exponential averaged quantities are not accurate enough.